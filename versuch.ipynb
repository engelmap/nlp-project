{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/dmitry/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "import csv\n",
    "import numpy as np\n",
    "import string\n",
    "import pandas as pd\n",
    "import re\n",
    "from string import digits\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "import spacy\n",
    "en_core = spacy.load('en_core_web_sm')\n",
    "nlp_lg = spacy.load('en_core_web_lg')\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [158]\u001B[0m, in \u001B[0;36m<cell line: 4>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m index, sent \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(data\u001B[38;5;241m.\u001B[39msentiment):\n\u001B[1;32m      5\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m sent \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfun\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m sent \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124menthusiasm\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m sent \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlove\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m sent \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhappiness\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m sent \u001B[38;5;241m==\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrelief\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m----> 6\u001B[0m         data\u001B[38;5;241m.\u001B[39mloc[index,[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msentiment\u001B[39m\u001B[38;5;124m'\u001B[39m]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpositive\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m      7\u001B[0m     \u001B[38;5;28;01melif\u001B[39;00m sent \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhate\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m sent \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124manger\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m sent \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mempty\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m sent \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mworry\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m sent \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msadness\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mor\u001B[39;00m sent \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mboredom\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m      8\u001B[0m         data\u001B[38;5;241m.\u001B[39mloc[index,[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msentiment\u001B[39m\u001B[38;5;124m'\u001B[39m]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnegative\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
      "File \u001B[0;32m~/PycharmProjects/nlp-project/venv/lib/python3.8/site-packages/pandas/core/indexing.py:712\u001B[0m, in \u001B[0;36m_LocationIndexer.__setitem__\u001B[0;34m(self, key, value)\u001B[0m\n\u001B[1;32m    710\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    711\u001B[0m     key \u001B[38;5;241m=\u001B[39m com\u001B[38;5;241m.\u001B[39mapply_if_callable(key, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobj)\n\u001B[0;32m--> 712\u001B[0m indexer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_get_setitem_indexer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    713\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_has_valid_setitem_indexer(key)\n\u001B[1;32m    715\u001B[0m iloc \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124miloc\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobj\u001B[38;5;241m.\u001B[39miloc\n",
      "File \u001B[0;32m~/PycharmProjects/nlp-project/venv/lib/python3.8/site-packages/pandas/core/indexing.py:661\u001B[0m, in \u001B[0;36m_LocationIndexer._get_setitem_indexer\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m    659\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(key, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[1;32m    660\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m suppress(IndexingError):\n\u001B[0;32m--> 661\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_convert_tuple\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkey\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    663\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(key, \u001B[38;5;28mrange\u001B[39m):\n\u001B[1;32m    664\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mlist\u001B[39m(key)\n",
      "File \u001B[0;32m~/PycharmProjects/nlp-project/venv/lib/python3.8/site-packages/pandas/core/indexing.py:799\u001B[0m, in \u001B[0;36m_LocationIndexer._convert_tuple\u001B[0;34m(self, key)\u001B[0m\n\u001B[1;32m    797\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_validate_key_length(key)\n\u001B[1;32m    798\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i, k \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(key):\n\u001B[0;32m--> 799\u001B[0m         idx \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_convert_to_indexer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mk\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mi\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    800\u001B[0m         keyidx\u001B[38;5;241m.\u001B[39mappend(idx)\n\u001B[1;32m    802\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mtuple\u001B[39m(keyidx)\n",
      "File \u001B[0;32m~/PycharmProjects/nlp-project/venv/lib/python3.8/site-packages/pandas/core/indexing.py:1244\u001B[0m, in \u001B[0;36m_LocIndexer._convert_to_indexer\u001B[0;34m(self, key, axis)\u001B[0m\n\u001B[1;32m   1241\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m labels\u001B[38;5;241m.\u001B[39m_convert_slice_indexer(key, kind\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mloc\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   1243\u001B[0m \u001B[38;5;66;03m# see if we are positional in nature\u001B[39;00m\n\u001B[0;32m-> 1244\u001B[0m is_int_index \u001B[38;5;241m=\u001B[39m \u001B[43mlabels\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mis_integer\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1245\u001B[0m is_int_positional \u001B[38;5;241m=\u001B[39m is_integer(key) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_int_index\n\u001B[1;32m   1247\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m is_scalar(key) \u001B[38;5;129;01mor\u001B[39;00m (\u001B[38;5;28misinstance\u001B[39m(labels, MultiIndex) \u001B[38;5;129;01mand\u001B[39;00m is_hashable(key)):\n\u001B[1;32m   1248\u001B[0m     \u001B[38;5;66;03m# Otherwise get_loc will raise InvalidIndexError\u001B[39;00m\n\u001B[1;32m   1249\u001B[0m \n\u001B[1;32m   1250\u001B[0m     \u001B[38;5;66;03m# if we are a label return me\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/nlp-project/venv/lib/python3.8/site-packages/pandas/core/indexes/base.py:2342\u001B[0m, in \u001B[0;36mIndex.is_integer\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   2308\u001B[0m \u001B[38;5;129m@final\u001B[39m\n\u001B[1;32m   2309\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mis_integer\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mbool\u001B[39m:\n\u001B[1;32m   2310\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   2311\u001B[0m \u001B[38;5;124;03m    Check if the Index only consists of integers.\u001B[39;00m\n\u001B[1;32m   2312\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   2340\u001B[0m \u001B[38;5;124;03m    False\u001B[39;00m\n\u001B[1;32m   2341\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 2342\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minferred_type\u001B[49m \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minteger\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "File \u001B[0;32m~/PycharmProjects/nlp-project/venv/lib/python3.8/site-packages/pandas/_libs/properties.pyx:37\u001B[0m, in \u001B[0;36mpandas._libs.properties.CachedProperty.__get__\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m~/PycharmProjects/nlp-project/venv/lib/python3.8/site-packages/pandas/core/indexes/base.py:2601\u001B[0m, in \u001B[0;36mIndex.inferred_type\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   2596\u001B[0m \u001B[38;5;129m@cache_readonly\u001B[39m\n\u001B[1;32m   2597\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21minferred_type\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m str_t:\n\u001B[1;32m   2598\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   2599\u001B[0m \u001B[38;5;124;03m    Return a string of the type inferred from the values.\u001B[39;00m\n\u001B[1;32m   2600\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 2601\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mlib\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minfer_dtype\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_values\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mskipna\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "#binary\n",
    "data = pd.read_csv(\"tweet_emotions.csv\")\n",
    "count = 0\n",
    "for index, sent in enumerate(data.sentiment):\n",
    "    if sent == \"fun\" or sent == \"enthusiasm\" or sent == \"love\" or sent == \"happiness\" or sent ==\"relief\":\n",
    "        data.loc[index,['sentiment']] = \"positive\"\n",
    "    elif sent == \"hate\" or sent == \"anger\" or sent == \"empty\" or sent == \"worry\" or sent == \"sadness\" or sent == \"boredom\":\n",
    "        data.loc[index,['sentiment']] = \"negative\"\n",
    "    else:\n",
    "        #if count % 6 != 0:\n",
    "        data.drop([index], axis=0, inplace=True)\n",
    "\n",
    "data.to_csv('tweet_emotions_binary.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [
    {
     "data": {
      "text/plain": "negative    16063\npositive    13112\nName: sentiment, dtype: int64"
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data\n",
    "data['sentiment'].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "outputs": [],
   "source": [
    "#three classes\n",
    "data = pd.read_csv(\"tweet_emotions.csv\")\n",
    "count = 0\n",
    "for index, sent in enumerate(data.sentiment):\n",
    "    if sent == \"fun\" or sent == \"enthusiasm\" or sent == \"love\" or sent == \"happiness\" or sent ==\"relief\":\n",
    "        data.loc[index,['sentiment']] = \"positive\"\n",
    "    elif sent == \"hate\" or sent == \"anger\" or sent == \"empty\" or sent == \"worry\" or sent == \"sadness\" or sent == \"boredom\":\n",
    "        data.loc[index,['sentiment']] = \"negative\"\n",
    "    elif sent == \"surprise\":\n",
    "        data.drop([index], axis=0, inplace=True)\n",
    "data.to_csv('tweet_emotions_three_classes.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "outputs": [
    {
     "data": {
      "text/plain": "negative    16063\npositive    13112\nneutral      8638\nName: sentiment, dtype: int64"
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data\n",
    "data['sentiment'].value_counts()\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Data cleaning functions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "\n",
    "def remove_punctuation(line_text):\n",
    "    line_text_without_punct = re.sub(r'[^\\w\\s]', '', line_text)\n",
    "    return line_text_without_punct\n",
    "\n",
    "\n",
    "def remove_numbers(line_text):\n",
    "    table = str.maketrans('', '', digits)\n",
    "    line_text_without_nums = line_text.translate(table)\n",
    "    return line_text_without_nums\n",
    "\n",
    "def remove_url(line_text):\n",
    "    #(https?:\\/\\/) matches http:// or https://\n",
    "    #(\\s)* optional whitespaces\n",
    "    #(www\\.)? optionally matches www.\n",
    "    # (\\s)* optionally matches whitespaces\n",
    "    #'((\\w|\\s)+\\.)* matches 0 or more of one or more word characters followed by a period\n",
    "    #([\\w\\-\\s]+\\/)* matches 0 or more of one or more words(or a dash or a space) followed by '\\'\n",
    "    #([\\w\\-]+) any remaining path at the end of the url followed by an optional ending\n",
    "    #((\\?)?[\\w\\s]*=\\s*[\\w\\%&]*)* matches ending query params (even with white spaces,etc)\n",
    "\n",
    "    line_text_without_url = re.sub(r'(https?:\\/\\/)(\\s)*(www\\.)?(\\s)*((\\w|\\s)+\\.)*([\\w\\-\\s]+\\/)*([\\w\\-]+)((\\?)?[\\w\\s]*=\\s*[\\w\\%&]*)*', '', line_text)\n",
    "    return line_text_without_url\n",
    "\n",
    "def remove_username(line_text):\n",
    "    line_text_without_username = re.sub(\"([@#][A-Za-z0-9_]+)|(\\w+:\\/\\/\\S+)\",'', line_text)\n",
    "    return line_text_without_username\n",
    "\n",
    "def remove_stopwords(line_text):\n",
    "    line_text_without_stopwords = ' '.join([word for word in line_text.split()\n",
    "                                           if word not in (stopwords.words('english'))])\n",
    "    return line_text_without_stopwords\n",
    "\n",
    "def translate_abbreviations(line_text):\n",
    "    text_list = line_text.split()\n",
    "    with open(\"slang_abbreviations.txt\", 'r') as my_csv_file:\n",
    "        data_from_file = csv.reader(my_csv_file, delimiter=\"=\")\n",
    "        for index, _str in enumerate(text_list):\n",
    "            _str = re.sub('[^a-zA-Z0-9-_.]', '', _str)\n",
    "            for row in data_from_file:\n",
    "                if _str.upper() == row[0]:\n",
    "                    text_list[index] = row[1]\n",
    "    final_string = ' '.join(text_list)\n",
    "    return final_string\n",
    "\n",
    "def remove_hashtags(line_text):\n",
    "    line_text_without_hashtags = line_text.replace('#','')\n",
    "    return line_text_without_hashtags\n",
    "\n",
    "def lemmatizing(line_text):\n",
    "    final_string = \" \".join([word.lemma_ for word in en_core(line_text)])\n",
    "    return final_string\n",
    "\n",
    "def clean_line(line_text):\n",
    "    text_zero = remove_numbers(line_text)\n",
    "    text_one = remove_username(text_zero)\n",
    "    text_two = remove_url(text_one)\n",
    "    text_three = remove_punctuation(text_two)\n",
    "    text_four = remove_hashtags(text_three)\n",
    "    text_five = remove_stopwords(text_four)\n",
    "    text_six = translate_abbreviations(text_five)\n",
    "    text_final = lemmatizing(text_six)\n",
    "    return text_final\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Cleaning the text and saving cleaned version to new column 'cleaned_content'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "df_multiple = pd.read_csv(\"tweet_emotions.csv\")\n",
    "df_binary = pd.read_csv(\"tweet_emotions_binary.csv\")\n",
    "df_three_classes = pd.read_csv(\"tweet_emotions_three_classes.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "df_multiple['cleaned_content'] = df_multiple.content.apply(clean_line)\n",
    "df_binary['cleaned_content'] = df_binary.content.apply(clean_line)\n",
    "df_three_classes['cleaned_content'] = df_three_classes.content.apply(clean_line)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Saving cleaned Dataframe to scv for future"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "df_binary.to_csv('tweet_emotions_binary_cleaned.csv')\n",
    "df_three_classes.to_csv('tweet_emotions_three_classes_cleaned.csv')\n",
    "df_multiple.to_csv('tweet_emotions_cleaned.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "outputs": [
    {
     "data": {
      "text/plain": "       Unnamed: 0    tweet_id sentiment  \\\n0               0  1956967341  negative   \n1               1  1956967666  negative   \n2               2  1956967696  negative   \n3               3  1956967789  positive   \n4               5  1956968477  negative   \n...           ...         ...       ...   \n29170       39994  1753918900  positive   \n29171       39996  1753919001  positive   \n29172       39997  1753919005  positive   \n29173       39998  1753919043  positive   \n29174       39999  1753919049  positive   \n\n                                                 content  \\\n0      @tiffanylue i know  i was listenin to bad habi...   \n1      Layin n bed with a headache  ughhhh...waitin o...   \n2                    Funeral ceremony...gloomy friday...   \n3                   wants to hang out with friends SOON!   \n4      Re-pinging @ghostridah14: why didn't you go to...   \n...                                                  ...   \n29170                      Succesfully following Tayla!!   \n29171                     Happy Mothers Day  All my love   \n29172  Happy Mother's Day to all the mommies out ther...   \n29173  @niariley WASSUP BEAUTIFUL!!! FOLLOW ME!!  PEE...   \n29174  @mopedronin bullet train from tokyo    the gf ...   \n\n                                         cleaned_content  \n0       know listenin bad habit early start freakin part  \n1                 Layin n bed headache ughhhhwaitin call  \n2                          funeral ceremonygloomy friday  \n3                                  want hang friend soon  \n4      repinging do not go prom BC bf do not like friend  \n...                                                  ...  \n29170                           succesfully follow tayla  \n29171                         happy Mothers Day all love  \n29172  happy Mothers Day mommie woman man long you re...  \n29173  wassup BEAUTIFUL FOLLOW I peep out my new HIT ...  \n29174  bullet train tokyo gf visit japan since thursd...  \n\n[29175 rows x 5 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>tweet_id</th>\n      <th>sentiment</th>\n      <th>content</th>\n      <th>cleaned_content</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>1956967341</td>\n      <td>negative</td>\n      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n      <td>know listenin bad habit early start freakin part</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>1956967666</td>\n      <td>negative</td>\n      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n      <td>Layin n bed headache ughhhhwaitin call</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>1956967696</td>\n      <td>negative</td>\n      <td>Funeral ceremony...gloomy friday...</td>\n      <td>funeral ceremonygloomy friday</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>1956967789</td>\n      <td>positive</td>\n      <td>wants to hang out with friends SOON!</td>\n      <td>want hang friend soon</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>1956968477</td>\n      <td>negative</td>\n      <td>Re-pinging @ghostridah14: why didn't you go to...</td>\n      <td>repinging do not go prom BC bf do not like friend</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>29170</th>\n      <td>39994</td>\n      <td>1753918900</td>\n      <td>positive</td>\n      <td>Succesfully following Tayla!!</td>\n      <td>succesfully follow tayla</td>\n    </tr>\n    <tr>\n      <th>29171</th>\n      <td>39996</td>\n      <td>1753919001</td>\n      <td>positive</td>\n      <td>Happy Mothers Day  All my love</td>\n      <td>happy Mothers Day all love</td>\n    </tr>\n    <tr>\n      <th>29172</th>\n      <td>39997</td>\n      <td>1753919005</td>\n      <td>positive</td>\n      <td>Happy Mother's Day to all the mommies out ther...</td>\n      <td>happy Mothers Day mommie woman man long you re...</td>\n    </tr>\n    <tr>\n      <th>29173</th>\n      <td>39998</td>\n      <td>1753919043</td>\n      <td>positive</td>\n      <td>@niariley WASSUP BEAUTIFUL!!! FOLLOW ME!!  PEE...</td>\n      <td>wassup BEAUTIFUL FOLLOW I peep out my new HIT ...</td>\n    </tr>\n    <tr>\n      <th>29174</th>\n      <td>39999</td>\n      <td>1753919049</td>\n      <td>positive</td>\n      <td>@mopedronin bullet train from tokyo    the gf ...</td>\n      <td>bullet train tokyo gf visit japan since thursd...</td>\n    </tr>\n  </tbody>\n</table>\n<p>29175 rows × 5 columns</p>\n</div>"
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_binary"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "function which vectorize text and give the train and test data back"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [],
   "source": [
    "def SpacyVec_data_from_df(df_input):\n",
    "    df = df_input\n",
    "    df['text_tokenized'] = df.clened_content.apply(lambda x: nlp_lg(x)) # tokenizing every text in the dataframe\n",
    "\n",
    "    df.sentiment = pd.Categorical(df.sentiment) # changing type of a column to categorical\n",
    "    df['Numeric_Labels'] = df.sentiment.cat.codes # saving category codes of column labels to new column\n",
    "\n",
    "    #doing sentence embedding before splitting, because the order makes no difference\n",
    "    df['text_vectorized'] = df['text_tokenized'].apply(lambda x: x.vector)\n",
    "\n",
    "    # splitting data _________________COULD BE CHANGED____________________________________________________\n",
    "    data_train, data_test, label_train, label_test = train_test_split(df.text_vectorized, df.Numeric_Labels,\n",
    "                                                                 test_size=0.30,\n",
    "                                                                 random_state=101, shuffle=True)\n",
    "   # __________________________________________________________________________________________________________\n",
    "\n",
    "    # appending all array rows into one and reshaping them from (1,300) shape to (300,1) shape\n",
    "    data_list_train = [vec.reshape(1,-1) for vec in data_train]\n",
    "    data_list_test = [vec.reshape(1,-1) for vec in data_test]\n",
    "\n",
    "    # concentrating array to get them readable by sklearn models\n",
    "    data_train_reshaped = np.concatenate(data_list_train)\n",
    "    data_test_reshaped = np.concatenate(data_list_test)\n",
    "\n",
    "    return data_train_reshaped, data_test_reshaped, label_train, label_test\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def TfIdf_data_from_df(df_input):\n",
    "    df = df_input\n",
    "    df.sentiment = pd.Categorical(df.sentiment) # changing type of a column to categorical\n",
    "    df['Numeric_Labels'] = df.sentiment.cat.codes # saving category codes of column labels to new column\n",
    "\n",
    "    # splitting data _________________COULD BE CHANGED____________________________________________________\n",
    "    data_train, data_test, label_train, label_test = train_test_split(df.cleaned_content, df.Numeric_Labels, test_size=0.30, random_state=101, shuffle=True)\n",
    "   # __________________________________________________________________________________________________________\n",
    "\n",
    "   # vectorizing test and train data with Tfidf\n",
    "    polarity_tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n",
    "    polarity_bow_matrix_train = polarity_tfidf_vectorizer.fit_transform(data_train)\n",
    "    polarity_bow_matrix_test = polarity_tfidf_vectorizer.transform(data_test)\n",
    "\n",
    "\n",
    "    return polarity_bow_matrix_train, polarity_bow_matrix_test, label_train, label_test\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "np.nan is an invalid document, expected byte or unicode string.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[0;32mIn [7]\u001B[0m, in \u001B[0;36m<cell line: 2>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m df_binary \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_csv(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtweet_emotions_binary_cleaned.csv\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m----> 2\u001B[0m data_train, data_test, label_train, label_test \u001B[38;5;241m=\u001B[39m \u001B[43mTfIdf_data_from_df\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf_binary\u001B[49m\u001B[43m)\u001B[49m\n",
      "Input \u001B[0;32mIn [2]\u001B[0m, in \u001B[0;36mTfIdf_data_from_df\u001B[0;34m(df_input)\u001B[0m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;66;03m# __________________________________________________________________________________________________________\u001B[39;00m\n\u001B[1;32m      9\u001B[0m \n\u001B[1;32m     10\u001B[0m \u001B[38;5;66;03m# vectorizing test and train data with Tfidf\u001B[39;00m\n\u001B[1;32m     11\u001B[0m  polarity_tfidf_vectorizer \u001B[38;5;241m=\u001B[39m TfidfVectorizer(ngram_range\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m))\n\u001B[0;32m---> 12\u001B[0m  polarity_bow_matrix_train \u001B[38;5;241m=\u001B[39m \u001B[43mpolarity_tfidf_vectorizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit_transform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata_train\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     13\u001B[0m  polarity_bow_matrix_test \u001B[38;5;241m=\u001B[39m polarity_tfidf_vectorizer\u001B[38;5;241m.\u001B[39mtransform(data_test)\n\u001B[1;32m     16\u001B[0m  \u001B[38;5;28;01mreturn\u001B[39;00m polarity_bow_matrix_train, polarity_bow_matrix_test, label_train, label_test\n",
      "File \u001B[0;32m~/PycharmProjects/nlp-project/venv/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:2078\u001B[0m, in \u001B[0;36mTfidfVectorizer.fit_transform\u001B[0;34m(self, raw_documents, y)\u001B[0m\n\u001B[1;32m   2071\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_params()\n\u001B[1;32m   2072\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tfidf \u001B[38;5;241m=\u001B[39m TfidfTransformer(\n\u001B[1;32m   2073\u001B[0m     norm\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnorm,\n\u001B[1;32m   2074\u001B[0m     use_idf\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39muse_idf,\n\u001B[1;32m   2075\u001B[0m     smooth_idf\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msmooth_idf,\n\u001B[1;32m   2076\u001B[0m     sublinear_tf\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msublinear_tf,\n\u001B[1;32m   2077\u001B[0m )\n\u001B[0;32m-> 2078\u001B[0m X \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit_transform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mraw_documents\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2079\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tfidf\u001B[38;5;241m.\u001B[39mfit(X)\n\u001B[1;32m   2080\u001B[0m \u001B[38;5;66;03m# X is already a transformed view of raw_documents so\u001B[39;00m\n\u001B[1;32m   2081\u001B[0m \u001B[38;5;66;03m# we set copy to False\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/nlp-project/venv/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:1338\u001B[0m, in \u001B[0;36mCountVectorizer.fit_transform\u001B[0;34m(self, raw_documents, y)\u001B[0m\n\u001B[1;32m   1330\u001B[0m             warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[1;32m   1331\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mUpper case characters found in\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1332\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m vocabulary while \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlowercase\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1333\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m is True. These entries will not\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1334\u001B[0m                 \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m be matched with any documents\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   1335\u001B[0m             )\n\u001B[1;32m   1336\u001B[0m             \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[0;32m-> 1338\u001B[0m vocabulary, X \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_count_vocab\u001B[49m\u001B[43m(\u001B[49m\u001B[43mraw_documents\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfixed_vocabulary_\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1340\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbinary:\n\u001B[1;32m   1341\u001B[0m     X\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mfill(\u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[0;32m~/PycharmProjects/nlp-project/venv/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:1209\u001B[0m, in \u001B[0;36mCountVectorizer._count_vocab\u001B[0;34m(self, raw_documents, fixed_vocab)\u001B[0m\n\u001B[1;32m   1207\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m doc \u001B[38;5;129;01min\u001B[39;00m raw_documents:\n\u001B[1;32m   1208\u001B[0m     feature_counter \u001B[38;5;241m=\u001B[39m {}\n\u001B[0;32m-> 1209\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m feature \u001B[38;5;129;01min\u001B[39;00m \u001B[43manalyze\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdoc\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[1;32m   1210\u001B[0m         \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1211\u001B[0m             feature_idx \u001B[38;5;241m=\u001B[39m vocabulary[feature]\n",
      "File \u001B[0;32m~/PycharmProjects/nlp-project/venv/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:106\u001B[0m, in \u001B[0;36m_analyze\u001B[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001B[0m\n\u001B[1;32m     84\u001B[0m \u001B[38;5;124;03m\"\"\"Chain together an optional series of text processing steps to go from\u001B[39;00m\n\u001B[1;32m     85\u001B[0m \u001B[38;5;124;03ma single document to ngrams, with or without tokenizing or preprocessing.\u001B[39;00m\n\u001B[1;32m     86\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    102\u001B[0m \u001B[38;5;124;03m    A sequence of tokens, possibly with pairs, triples, etc.\u001B[39;00m\n\u001B[1;32m    103\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    105\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m decoder \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 106\u001B[0m     doc \u001B[38;5;241m=\u001B[39m \u001B[43mdecoder\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdoc\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    107\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m analyzer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    108\u001B[0m     doc \u001B[38;5;241m=\u001B[39m analyzer(doc)\n",
      "File \u001B[0;32m~/PycharmProjects/nlp-project/venv/lib/python3.8/site-packages/sklearn/feature_extraction/text.py:234\u001B[0m, in \u001B[0;36m_VectorizerMixin.decode\u001B[0;34m(self, doc)\u001B[0m\n\u001B[1;32m    231\u001B[0m     doc \u001B[38;5;241m=\u001B[39m doc\u001B[38;5;241m.\u001B[39mdecode(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mencoding, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdecode_error)\n\u001B[1;32m    233\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m doc \u001B[38;5;129;01mis\u001B[39;00m np\u001B[38;5;241m.\u001B[39mnan:\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    235\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnp.nan is an invalid document, expected byte or unicode string.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    236\u001B[0m     )\n\u001B[1;32m    238\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m doc\n",
      "\u001B[0;31mValueError\u001B[0m: np.nan is an invalid document, expected byte or unicode string."
     ]
    }
   ],
   "source": [
    "df_binary = pd.read_csv('tweet_emotions_binary_cleaned.csv')\n",
    "data_train, data_test, label_train, label_test = TfIdf_data_from_df(df_binary)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [],
   "source": [
    "forest_classifier = RandomForestClassifier()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluate MLPClassifier:\n",
      "\n",
      "Accuracy: 0.330\n",
      "\n",
      "Confusion Matrix: \n"
     ]
    },
    {
     "data": {
      "text/plain": "array([[[11966,     0],\n        [   34,     0]],\n\n       [[11946,     4],\n        [   50,     0]],\n\n       [[11704,    33],\n        [  261,     2]],\n\n       [[11775,    13],\n        [  212,     0]],\n\n       [[11390,    81],\n        [  513,    16]],\n\n       [[ 9408,  1001],\n        [ 1109,   482]],\n\n       [[11519,    86],\n        [  341,    54]],\n\n       [[10323,   589],\n        [  684,   404]],\n\n       [[ 6056,  3322],\n        [ 1055,  1567]],\n\n       [[11442,    69],\n        [  471,    18]],\n\n       [[ 9867,   517],\n        [ 1292,   324]],\n\n       [[11250,   122],\n        [  600,    28]],\n\n       [[ 7316,  2201],\n        [ 1416,  1067]]])"
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest_classifier = RandomForestClassifier()\n",
    "forest_classifier.fit(data_train, label_train)\n",
    "test_forest= forest_classifier.predict(data_test)\n",
    "\n",
    "#evaluationg model and reporting accuracy and multi-class confusion matrix\n",
    "print('\\nEvaluate MLPClassifier:')\n",
    "print('\\nAccuracy: %.3f' % metrics.accuracy_score(label_test, test_forest))\n",
    "print('\\nConfusion Matrix: ')\n",
    "multilabel_confusion_matrix(label_test, test_forest)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_models(df, vectorizer, text=\"text\", labels=\"emotion\"):\n",
    "    classifiers = [svm.LinearSVC(), RandomForestClassifier()] #DecisionTreeClassifier()]# # MLPClassifier()]\n",
    "\n",
    "    #look into learning curves (do we have enough samples?) / validation curves (is the model under/overfitting?)\n",
    "\n",
    "    X = vect.transform(df[text])\n",
    "    y = df[labels]\n",
    "\n",
    "    print(\"Amount of features: \", len(vect.get_feature_names_out()))\n",
    "    print(\"Features: \", vect.get_feature_names_out())\n",
    "\n",
    "    cv = ShuffleSplit(n_splits=5, test_size=0.2, random_state=0)\n",
    "    plot_learning_curve(classifiers, X, y, ylim=(0.5, 1.01), cv=cv, n_jobs=4)\n",
    "\n",
    "    results = calc_metrics(classifiers, X, y)\n",
    "\n",
    "    df_results = pd.DataFrame(results)\n",
    "    return df_results\n",
    "\n",
    "    plt.show("
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f31c1fe1c36aaa4d32cb0816fd78552f0caa11e77dcf57aa2899123b61a94662"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}